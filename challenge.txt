Assess technical skills in applied AI, mastery of deep learning frameworks, knowledge of generative models (GANs and Diffusion Models), and best practices in machine learning engineering.

Desired Environment
Using Python and Jupyter Notebook, develop a project that generates data from scratch using generative models. The candidate can freely choose the approach and framework to be used (PyTorch, TensorFlow, or equivalent).

Part 1 — Image Generation
Description
Implement a generative model capable of producing synthetic images from a simple dataset, such as MNIST, using GANs or Diffusion Models.

You must:

• Implement a generative model capable of producing images based on a dataset of your choice (e.g., MNIST).

• Train the model and generate samples, presenting comparative results between different training epochs.

• Implement quality metrics (FID, IS, or similar) and present visual and numerical analysis of the results.

• Save the model weights and demonstrate loading for inference.

• Optionally, include a simple interface (CLI or web) that allows generating new images from the trained model.

Part 2 — Audio Regeneration with MUSDB
Objective
To evaluate the candidate's ability to apply deep learning models to audio signals, demonstrating an understanding of preprocessing, generative modeling, and quality assessment in a domain other than images.

Description
Using the MUSDB18 dataset, develop a simple audio regeneration experiment. The goal is to train a model capable of reconstructing or regenerating an audio stem (e.g., vocals, drums, bass, or other) from its time-frequency domain representation (e.g., spectrogram or mel-spectrogram).

The candidate has the freedom to choose the model architecture (e.g., Autoencoder, VAE, GAN, or Diffusion Model), as well as the deep learning framework.

Requirements:

• Use the MUSDB18 dataset or one of its public variants.

• Perform audio preprocessing (e.g., STFT or Mel-Spectrogram).

• Implement a model capable of regenerating audio from this representation.

• Train the model and present examples of reconstructed/regenerated audio.

• Compare the original audio and the reconstructed audio qualitatively and quantitatively.

• Save the model weights and demonstrate loading for inference.

What should be delivered?

• Source code and notebooks hosted in a public repository on GitHub.

• A README.md file containing:
o Project description and justification of the chosen architectures.
o Frameworks and libraries used.

Instructions for running the training and inference (images and, optionally, audio).

Evaluation metrics and main results obtained.