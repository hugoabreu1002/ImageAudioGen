# ImageAudioGen

Projeto completo de intelig√™ncia artificial generativa que implementa dois modelos de deep learning: gera√ß√£o de imagens com Diffusion Models e regenera√ß√£o de √°udio com Autoencoders.

## üìã Descri√ß√£o do Projeto

Este projeto avalia capacidades t√©cnicas em IA aplicada, demonstrando:
- Dom√≠nio de frameworks de deep learning (PyTorch)
- Conhecimento de modelos generativos (Diffusion Models e Autoencoders)
- Boas pr√°ticas de engenharia de machine learning
- Pr√©-processamento e avalia√ß√£o de dados

## üèóÔ∏è Arquitetura do Projeto

```
ImageAudioGen/
‚îú‚îÄ‚îÄ image_gen.py        # Gera√ß√£o de imagens com Diffusion Model
‚îú‚îÄ‚îÄ audio_gen.py        # Regenera√ß√£o de √°udio com Autoencoder
‚îú‚îÄ‚îÄ README.md           # Este arquivo
‚îî‚îÄ‚îÄ challenge.txt       # Descri√ß√£o dos requisitos
```

---

## üì∏ Parte 1: Gera√ß√£o de Imagens (`image_gen.py`)

### O que faz?
Implementa um **Diffusion Model** completo capaz de gerar imagens sint√©ticas de d√≠gitos manuscritos (MNIST) a partir de ru√≠do Gaussiano aleat√≥rio.

### Componentes Principais

#### 1. **PositionalEncoding**
- Codifica informa√ß√£o temporal (timesteps) da difus√£o
- Usa fun√ß√µes seno/cosseno para criar embeddings posicionais
- Permite que a rede entenda em qual est√°gio do processo de difus√£o est√°

#### 2. **DiffusionModel (U-Net)**
- **Entrada**: Imagem com ru√≠do + timestep
- **Sa√≠da**: Predi√ß√£o do ru√≠do Gaussiano adicionado
- **Arquitetura**: U-Net simplificada com:
  - Encoder: 2 blocos de convolu√ß√£o + maxpooling (reduz dimensionalidade)
  - Decoder: 2 blocos de deconvolu√ß√£o (restaura tamanho original)
  - Skip connections: Concatenam features do encoder com decoder

#### 3. **DiffusionTrainer**
- **Forward Diffusion**: Adiciona ru√≠do progressivamente √† imagem (1000 timesteps)
- **Reverse Diffusion**: Remove ru√≠do iterativamente para gerar novas imagens
- **Perda**: MSE entre ru√≠do predito e ru√≠do real

#### 4. **M√©tricas e Avalia√ß√£o**
- **FID (Fr√©chet Inception Distance)**: Mede qualidade/diversidade das imagens geradas
- Compara√ß√£o visual entre √©pocas
- Hist√≥rico de perda de treinamento

### Como Usar

**Treinar o modelo:**
```bash
python image_gen.py --mode train \
    --epochs 20 \
    --batch_size 64 \
    --learning_rate 1e-3 \
    --device cuda
```

**Gerar amostras com modelo treinado:**
```bash
python image_gen.py --mode infer \
    --num_samples 16 \
    --checkpoint models/diffusion_model.pt \
    --device cuda
```

### Argumentos Dispon√≠veis

| Argumento | Tipo | Padr√£o | Descri√ß√£o |
|-----------|------|--------|-----------|
| `--mode` | str | train | 'train' para treinar ou 'infer' para gerar |
| `--epochs` | int | 20 | N√∫mero de √©pocas de treinamento |
| `--batch_size` | int | 64 | Tamanho do batch |
| `--learning_rate` | float | 1e-3 | Taxa de aprendizado |
| `--checkpoint` | str | models/diffusion_model.pt | Caminho do modelo salvo |
| `--num_samples` | int | 16 | Quantidade de imagens a gerar |
| `--device` | str | cuda/cpu | CPU ou GPU |

### Outputs Gerados

```
results/
‚îú‚îÄ‚îÄ training_loss.png           # Gr√°fico de perda vs √©poca
‚îú‚îÄ‚îÄ epoch_comparisons/
‚îÇ   ‚îú‚îÄ‚îÄ samples_epoch_001.png   # Amostras na √©poca 1
‚îÇ   ‚îú‚îÄ‚îÄ samples_epoch_005.png   # Amostras na √©poca 5
‚îÇ   ‚îú‚îÄ‚îÄ samples_epoch_010.png   # Amostras na √©poca 10
‚îÇ   ‚îú‚îÄ‚îÄ samples_epoch_020.png   # Amostras na √©poca 20
‚îÇ   ‚îî‚îÄ‚îÄ metrics_comparison.png  # FID vs Loss vs √âpoca
‚îî‚îÄ‚îÄ final_samples.png           # Amostras finais geradas

models/
‚îî‚îÄ‚îÄ diffusion_model.pt          # Pesos do modelo treinado
```

### Resultados Esperados

- **Qualidade**: Imagens cada vez melhores conforme aumenta o treinamento
- **Diversidade**: FID aumenta (maior desvio padr√£o = mais diversidade)
- **Perda**: Diminui exponencialmente nas primeiras √©pocas

---

## üîä Parte 2: Regenera√ß√£o de √Åudio (`audio_gen.py`)

### O que faz?
Implementa um **Autoencoder** para reconstruir stems de √°udio a partir de representa√ß√µes no dom√≠nio tempo-frequ√™ncia (Mel-Spectrogram).

### Componentes Principais

#### 1. **AudioPreprocessor**
- Converte √°udio em **Mel-Spectrogram** (an√°lise em frequ√™ncia)
- **Mel-Spectrogram**: Representa√ß√£o que imita como o ouvido humano percebe som
  - Frequ√™ncias: Representadas em escala logar√≠tmica (Mel)
  - Eixo Y: 128 Mel bins (padr√£o)
  - Eixo X: Frames de tempo
- Usa **Griffin-Lim** para reconstruir √°udio a partir do Mel-Spectrogram
- Normaliza dados para treinamento

#### 2. **SyntheticMUSDBDataset**
- Simula dataset MUSDB18 (um padr√£o em separa√ß√£o de √°udio)
- Gera √°udio sint√©tico com:
  - M√∫ltiplas frequ√™ncias harm√¥nicas (440Hz, 880Hz, 1320Hz, 1760Hz)
  - Amplitudes vari√°veis
  - Ru√≠do Gaussiano
- Redimensiona para tamanho fixo (256 frames temporais)

#### 3. **AudioAutoencoder**
- **Entrada**: Mel-Spectrogram [128 mels √ó 256 timesteps]
- **Processo**:
  1. Encoder (4 camadas): Comprime para espa√ßo latente (64 dimens√µes)
  2. Bottleneck: Representa√ß√£o comprimida
  3. Decoder (4 camadas): Reconstr√≥i Mel-Spectrogram original
- **Sa√≠da**: Mel-Spectrogram reconstru√≠do [128 √ó 256]

#### 4. **AudioTrainer**
- **Fun√ß√£o de Perda**: MSE entre Mel-Spectrograms original e reconstru√≠do
- **Otimizador**: Adam com learning rate scheduler
- **Gradient Clipping**: Evita explos√£o de gradientes

#### 5. **M√©tricas de Qualidade**

| M√©trica | Descri√ß√£o | Intervalo |
|---------|-----------|-----------|
| **MSE** | Erro quadr√°tico m√©dio pixel a pixel | 0-‚àû (menor=melhor) |
| **MAE** | Erro absoluto m√©dio | 0-‚àû (menor=melhor) |
| **Cosine Similarity** | Similaridade entre espectros | 0-1 (maior=melhor) |
| **PESQ Proxy** | Aproxima√ß√£o de qualidade perceptual | 0-1 (maior=melhor) |

### Como Usar

**Treinar o modelo:**
```bash
python audio_gen.py --mode train \
    --epochs 30 \
    --batch_size 32 \
    --learning_rate 1e-3 \
    --num_samples 100 \
    --n_mels 128 \
    --latent_dim 64 \
    --device cuda
```

**Reconstruir √°udio com modelo treinado:**
```bash
python audio_gen.py --mode infer \
    --checkpoint models/audio_autoencoder.pt \
    --device cuda
```

### Argumentos Dispon√≠veis

| Argumento | Tipo | Padr√£o | Descri√ß√£o |
|-----------|------|--------|-----------|
| `--mode` | str | train | 'train' para treinar ou 'infer' para reconstruir |
| `--epochs` | int | 30 | N√∫mero de √©pocas de treinamento |
| `--batch_size` | int | 32 | Tamanho do batch |
| `--learning_rate` | float | 1e-3 | Taxa de aprendizado |
| `--num_samples` | int | 100 | Quantidade de amostras do dataset |
| `--checkpoint` | str | models/audio_autoencoder.pt | Caminho do modelo salvo |
| `--n_mels` | int | 128 | N√∫mero de Mel bins |
| `--latent_dim` | int | 64 | Dimens√£o do espa√ßo latente |
| `--device` | str | cuda/cpu | CPU ou GPU |

### Outputs Gerados

```
results/
‚îú‚îÄ‚îÄ training_curves.png                # Perda + M√©tricas vs √âpoca
‚îú‚îÄ‚îÄ spectrogram_comparison.png         # Original vs Reconstru√≠do
‚îú‚îÄ‚îÄ inference_comparison.png           # Amostras de teste
‚îú‚îÄ‚îÄ audio_reconstructed_0.wav          # √Åudio reconstru√≠do #0
‚îî‚îÄ‚îÄ audio_reconstructed_1.wav          # √Åudio reconstru√≠do #1

models/
‚îî‚îÄ‚îÄ audio_autoencoder.pt               # Pesos do modelo treinado
```

### Fluxo de Processamento

```
√Åudio original (16kHz, 5 segundos)
         ‚Üì
   Mel-Spectrogram
   [1 √ó 128 √ó 256]
         ‚Üì
    ENCODER
   (4 camadas Conv1d)
         ‚Üì
   Espa√ßo Latente
   [1 √ó 64]
         ‚Üì
    DECODER
   (4 camadas ConvTranspose1d)
         ‚Üì
   Mel-Spectrogram Reconstru√≠do
   [1 √ó 128 √ó 256]
         ‚Üì
Griffin-Lim Inverse
         ‚Üì
√Åudio Reconstru√≠do (16kHz)
```

### Resultados Esperados

- **MSE**: Decresce durante treinamento (come√ßa ~0.5, fim ~0.05)
- **Similaridade**: Aumenta (come√ßa ~0.5, fim ~0.95)
- **Qualidade Perceptual**: √Åudio reconstru√≠do cada vez mais fiel ao original

---

## üöÄ Como Executar

### Pr√©-requisitos

```bash
pip install torch torchaudio torchvision torchmetrics numpy matplotlib soundfile tqdm
```

### Execu√ß√£o Completa (Imagem + √Åudio)

```bash
# Passo 1: Treinar modelo de gera√ß√£o de imagens
python image_gen.py --mode train --epochs 20 --batch_size 64

# Passo 2: Gerar novas imagens
python image_gen.py --mode infer --num_samples 16

# Passo 3: Treinar modelo de regenera√ß√£o de √°udio
python audio_gen.py --mode train --epochs 30 --batch_size 32

# Passo 4: Reconstruir √°udio
python audio_gen.py --mode infer
```

---

## üìä M√©tricas e Resultados

### Diffusion Model (Imagens)
- **M√©trica Principal**: FID (Fr√©chet Inception Distance)
- **Visualiza√ß√£o**: Compara√ß√£o de amostras entre √©pocas
- **Loss**: MSE entre ru√≠do predito e real

### Autoencoder (√Åudio)
- **M√©tricas**: MSE, MAE, Cosine Similarity, PESQ Proxy
- **Visualiza√ß√£o**: Espectrogramas original vs reconstru√≠do
- **An√°lise**: Gr√°ficos de converg√™ncia

---

## üéØ Destaques T√©cnicos

‚úÖ **Modelos Generativos**: Diffusion Models (SOTA em gera√ß√£o de imagens)  
‚úÖ **Modelos Autoencoders**: Compress√£o e reconstru√ß√£o eficiente  
‚úÖ **Pr√©-processamento**: Mel-Spectrogram para √°udio, Normaliza√ß√£o para ambos  
‚úÖ **M√©tricas Avan√ßadas**: FID, Cosine Similarity, PESQ Proxy  
‚úÖ **Best Practices**: Checkpointing, Learning Rate Scheduling, Gradient Clipping  
‚úÖ **CLI Intuitiva**: Argumentos configur√°veis para f√°cil experimenta√ß√£o  
‚úÖ **Visualiza√ß√£o**: Gr√°ficos comparativos e an√°lise de qualidade  

---

## üìù Frameworks Utilizados

- **PyTorch**: Deep learning framework principal
- **Torchaudio**: Processamento de √°udio
- **Torchvision**: Transforma√ß√µes de imagem
- **Matplotlib**: Visualiza√ß√£o
- **Soundfile**: Exporta√ß√£o de √°udio
- **Tqdm**: Barras de progresso

---

## üìö Refer√™ncias

- Ho et al. (2020): Denoising Diffusion Probabilistic Models (DDPM)
- Kingma & Welling (2013): Auto-Encoding Variational Bayes
- Mel-Frequency Cepstral Coefficients (MFCC) - Padr√£o em processamento de √°udio

---

## ‚ú® Pr√≥ximos Passos

- Implementar VAE (Variational Autoencoder) para √°udio
- Adicionar GAN para gera√ß√£o de imagens
- Integra√ß√£o com dados MUSDB18 reais
- API REST para infer√™ncia
- Web interface com Streamlit